# Waitress configuration for Windows/cross-platform deployment
from waitress import serve
from main import app
import os

def run_server():
    """Run the Face Finder API with Waitress WSGI server"""
    
    # Configuration
    host = os.getenv('HOST', '0.0.0.0')
    port = int(os.getenv('PORT', 5003))
    threads = int(os.getenv('THREADS', 8))
    
    print(f"ğŸš€ Starting Face Finder API with Waitress...")
    print(f"ğŸ“¡ Server: http://{host}:{port}")
    print(f"ğŸ§µ Threads: {threads}")
    print(f"ğŸ’¾ Cache directory: cache/embeddings/")
    print(f"ğŸ“ Data directory: data/")
    print()
    print("Available endpoints:")
    print("ğŸ”„ POST /api/v0/embed - Pre-warm cache with image URLs")
    print("ğŸ” POST /api/v0/findIn - Find target in scope of images") 
    print("ğŸ” POST /api/findIn - Find matches in data directory (legacy)")
    print("ğŸ’š GET /api/health - Health check")
    print("ğŸ“Š GET /api/cache/stats - Cache statistics")
    print("ğŸ§¹ POST /api/cache/clear - Clear all cached embeddings")
    print("ğŸ§¹ POST /api/cache/cleanup - Remove invalid cache entries")
    print()
    
    # Ensure directories exist
    os.makedirs('cache/embeddings', exist_ok=True)
    os.makedirs('data', exist_ok=True)
    
    # Start server
    serve(
        app,
        host=host,
        port=port,
        threads=threads,
        url_scheme='http',
        # Performance settings
        cleanup_interval=30,
        channel_timeout=120,
        # Adjust for face processing workloads
        recv_bytes=65536,
        send_bytes=65536,
        # Connection settings
        backlog=1024,
        # Logging
        _quiet=False
    )

if __name__ == '__main__':
    run_server()
